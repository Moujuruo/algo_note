{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pickle as pkl\n",
    "# use AdamW is a standard practice for transformer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "# use Adafactor is the default setting for T5\n",
    "from transformers.optimization import Adafactor\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.prompts import SoftTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.data_utils.data_processor import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"\")\n",
    "parser.add_argument(\"--shot\", type=int, default=-1) # few-shot learning，-1 means full-shot\n",
    "parser.add_argument(\"--seed\", type=int, default=144)\n",
    "parser.add_argument(\n",
    "    \"--plm_eval_mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to turn off the dropout in the freezed model. Set to true to turn off.\")\n",
    "# turn off the dropout aims to compare different methods under the same setting\n",
    "parser.add_argument(\"--tune_plm\", action=\"store_true\")\n",
    "parser.add_argument(\n",
    "    \"--model\",\n",
    "    type=str,\n",
    "    default='t5',\n",
    "    help=\"We test both t5 and t5-lm in this scripts, the corresponding tokenizerwrapper will be automatically loaded.\") # the model to be tested\n",
    "parser.add_argument(\"--model_name_or_path\", default='t5-small')\n",
    "parser.add_argument(\"--template_id\", type=int, default=0)\n",
    "parser.add_argument(\"--verbalizer_id\", type=int, default=0) # verbalizer connects the label space and the prompt space\n",
    "parser.add_argument(\"--dataset\", type=str, default='HateXplain')\n",
    "parser.add_argument(\"--result_file\", type=str, default=\"sfs_out/results.txt\")\n",
    "parser.add_argument(\"--max_steps\", default=2000, type=int)\n",
    "parser.add_argument(\"--prompt_lr\", type=float, default=0.3)\n",
    "parser.add_argument(\"--warmup_step_prompt\", type=int, default=100)\n",
    "parser.add_argument(\"--init_from_vocab\", action=\"store_false\")\n",
    "parser.add_argument(\"--eval_every_steps\", type=int, default=200)\n",
    "parser.add_argument(\"--soft_token_num\", type=int, default=20) # signify the number of soft tokens in soft template\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"Adafactor\")\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataProcessor(DataProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.labels = [\"No\", \"Yes\"]\n",
    "\n",
    "    def random_sampling(self, dataset, sample_num): # sample a subset of the dataset\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        # if exceed training data size, set to training data size, then;\n",
    "        sample_num = min(sample_num, len(dataset[\"text\"]))\n",
    "        index_list = list(range(len(dataset[\"text\"])))\n",
    "        random.shuffle(index_list)\n",
    "        selected_index_list = index_list[:sample_num]\n",
    "        new_dataset = {\n",
    "            \"id\": [dataset[\"id\"][i] for i in selected_index_list],\n",
    "            \"text\": [dataset[\"text\"][i] for i in selected_index_list],\n",
    "            \"label\": [dataset[\"label\"][i] for i in selected_index_list]\n",
    "        }\n",
    "        return new_dataset\n",
    "\n",
    "    def get_examples(self, data_dir, split): # load the dataset\n",
    "        if split == \"valid\" or split == \"dev\": # unify the name of validation set\n",
    "            split = \"validation\" \n",
    "\n",
    "        dataset = json.loads(open(data_dir).read())\n",
    "        dataset = dataset[split]\n",
    "        # sample_num = 1000\n",
    "        sample_num = -1\n",
    "        if sample_num != -1 and split == \"train\":\n",
    "            dataset = self.random_sampling(dataset, sample_num)\n",
    "            print(\"%s, sample %d data.\" % (split, len(dataset[\"id\"])))\n",
    "        return self.transform(dataset)\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        res = []\n",
    "        for i in range(len(dataset[\"text\"])):\n",
    "            text_a = dataset['text'][i]\n",
    "            label = int(dataset['label'][i])\n",
    "            # for hateXplain\n",
    "            if label == 2:\n",
    "                label = 1\n",
    "            guid = \"{}\".format(dataset['id'][i])\n",
    "\n",
    "            res.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
    "            # create an InputExample object for each data point\n",
    "            # guid: a unique id for each data point; text_a: the input text; label: the label\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the specified pre-trained language model (such as T5) and its corresponding tokenizer, model configuration and WrapperClass\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "# specify the supported dataset list\n",
    "dataset_list = [\n",
    "    \"HateXplain\",\n",
    "    \"USElectionHate20\",\n",
    "    \"HateCheck\",\n",
    "    \"SBIC.v2\",\n",
    "    \"measuring-hate-speech\"]\n",
    "\n",
    "if args.dataset in dataset_list:\n",
    "    data_dir = \"parsed_dataset/%s_perspective_balance.json\" % (args.dataset)\n",
    "    \n",
    "    Processor = MyDataProcessor\n",
    "    # load the dataset and split it into training, validation and test sets    \n",
    "    dataset['train'] = Processor().get_train_examples(data_dir)\n",
    "    dataset['validation'] = Processor().get_dev_examples(data_dir)\n",
    "    dataset['test'] = Processor().get_test_examples(data_dir)\n",
    "    \n",
    "    # get the class labels\n",
    "    class_labels = Processor().get_labels()\n",
    "    \n",
    "    max_seq_l = 480\n",
    "    \n",
    "    # if you want to fine-tune the whole pre-trained language model, you need more GPU memory, so you need to use a smaller batch size\n",
    "    if args.tune_plm:\n",
    "        batchsize_t = 4\n",
    "        batchsize_e = 4\n",
    "        gradient_accumulation_steps = 8\n",
    "        model_parallelize = True\n",
    "    else:\n",
    "        if args.model_name_or_path == \"gpt2-large\":\n",
    "            batchsize_t = 4\n",
    "            batchsize_e = 4\n",
    "            gradient_accumulation_steps = 8\n",
    "            model_parallelize = False\n",
    "        elif args.model_name_or_path == \"gpt2-medium\":\n",
    "            batchsize_t = 4\n",
    "            batchsize_e = 4\n",
    "            gradient_accumulation_steps = 8\n",
    "            model_parallelize = False\n",
    "        elif args.model_name_or_path == \"gpt2-xl\":\n",
    "            batchsize_t = 2\n",
    "            batchsize_e = 2\n",
    "            gradient_accumulation_steps = 16\n",
    "            model_parallelize = False\n",
    "        else:\n",
    "            batchsize_t = 8\n",
    "            batchsize_e = 8\n",
    "            gradient_accumulation_steps = 4\n",
    "            model_parallelize = False\n",
    "else:\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytemplate = SoftTemplate(\n",
    "    model=plm,\n",
    "    tokenizer=tokenizer,\n",
    "    num_tokens=args.soft_token_num,\n",
    "    initialize_from_vocab=args.init_from_vocab).from_file(\n",
    "        \"experiment_scripts/soft_template/soft_template.txt\",\n",
    "    choice=0) # load the soft template, which is a set of soft tokens\n",
    "# soft tmplate looks like: \"This is a [MASK] sentence.\" [MASK] is a soft token\n",
    "\n",
    "myverbalizer = ManualVerbalizer(\n",
    "    tokenizer,\n",
    "    classes=class_labels).from_file(\n",
    "        \"experiment_scripts/soft_template/manual_verbalizer.txt\",\n",
    "    choice=args.verbalizer_id)\n",
    "# verbalizer connects the label space and the prompt space\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "# wrap_one_example is a function that wraps the input example with the soft template and the verbalizer\n",
    "print(wrapped_example)\n",
    "\n",
    "# following is the training process,\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(\n",
    "    plm=plm,\n",
    "    template=mytemplate,\n",
    "    verbalizer=myverbalizer,\n",
    "    freeze_plm=(\n",
    "        not args.tune_plm),\n",
    "    plm_eval_mode=args.plm_eval_mode)\n",
    "if use_cuda:\n",
    "    prompt_model = prompt_model.cuda()\n",
    "\n",
    "# following load Prompt-based Learning data loader\n",
    "if model_parallelize:\n",
    "    prompt_model.parallelize() # parallelize the model to use multiple GPUs\n",
    "    train_dataloader = PromptDataLoader(\n",
    "    dataset=dataset[\"train\"],\n",
    "    template=mytemplate,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=max_seq_l,\n",
    "    decoder_max_length=3,\n",
    "    batch_size=batchsize_t,\n",
    "    shuffle=True,\n",
    "    teacher_forcing=False,\n",
    "    predict_eos_token=False,\n",
    "    truncate_method=\"tail\") \n",
    "\n",
    "test_dataloader = PromptDataLoader(\n",
    "    dataset=dataset[\"test\"],\n",
    "    template=mytemplate,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass,\n",
    "    max_seq_length=max_seq_l,\n",
    "    decoder_max_length=3,\n",
    "    batch_size=batchsize_e,\n",
    "    shuffle=False,\n",
    "    teacher_forcing=False,\n",
    "    predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "\n",
    "print(\n",
    "    \"truncate rate: {}\".format(\n",
    "        test_dataloader.tokenizer_wrapper.truncate_rate),\n",
    "    flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following calulate the accuracy, precision, recall and f1 score\n",
    "def evaluate(prompt_model, dataloader, desc, return_data=False):\n",
    "    prompt_model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    prompt_model.train()\n",
    "    acc = accuracy_score(alllabels, allpreds) \n",
    "    p = precision_score(alllabels, allpreds) \n",
    "    r = recall_score(alllabels, allpreds) \n",
    "    f1 = f1_score(alllabels, allpreds)\n",
    "    res = [acc, p, r, f1]\n",
    "    if not return_data:\n",
    "        return res\n",
    "    elif return_data:\n",
    "        return res, alllabels, allpreds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# if tune_plm is True, fine-tune the pre-trained language model\n",
    "if args.tune_plm:\n",
    "    # define the parameters that need to be optimized\n",
    "    # group 1: the parameters in the pre-trained language model that do not contain \"bias\" and \"LayerNorm.weight\", these parameters use weight_decay of 0.01\n",
    "    # group 2: the parameters in the pre-trained language model that contain \"bias\" and \"LayerNorm.weight\", these parameters use weight_decay of 0.0\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters1 = [\n",
    "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    # use AdamW optimizer, learning rate is 3e-5\n",
    "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "    \n",
    "    # define the learning rate scheduler, using linear decay strategy\n",
    "    # parameters:\n",
    "    #   - optimizer1: optimizer\n",
    "    #   - num_warmup_steps: warm-up steps, set to 500 here\n",
    "    #   - num_training_steps: total training steps, i.e. args.max_steps\n",
    "    scheduler1 = get_linear_schedule_with_warmup(optimizer1, num_warmup_steps=500, num_training_steps=args.max_steps)\n",
    "else:\n",
    "    # if tune_plm is False, do not fine-tune the parameters of the pre-trained language model\n",
    "    optimizer1 = None\n",
    "    scheduler1 = None\n",
    "\n",
    "# define the parameters that need to be optimized\n",
    "# here, the \"raw_embedding\" parameter is excluded because it does not need to be optimized\n",
    "optimizer_grouped_parameters2 = [{'params': [p for name, p in prompt_model.template.named_parameters() if 'raw_embedding' not in name]}]\n",
    "\n",
    "if args.optimizer.lower() == \"adafactor\":\n",
    "    # 使用 Adafactor 优化器\n",
    "    # 参数:\n",
    "    #   - optimizer_grouped_parameters2: 需要优化的参数组\n",
    "    #   - lr: 学习率,这里设置为 args.prompt_lr,即 Prompt 的学习率\n",
    "    #   - relative_step: 是否使用相对步数,这里设置为 False\n",
    "    #   - scale_parameter: 是否对参数进行缩放,这里设置为 False\n",
    "    #   - warmup_init: 是否在初始化时进行预热,这里设置为 False\n",
    "    optimizer2 = Adafactor(optimizer_grouped_parameters2, lr=args.prompt_lr, relative_step=False, scale_parameter=False, warmup_init=False)\n",
    "    \n",
    "    # 定义学习率调度器,使用常数预热策略\n",
    "    # 参数:\n",
    "    #   - optimizer2: 优化器\n",
    "    #   - num_warmup_steps: 预热步数,这里设置为 args.warmup_step_prompt\n",
    "    scheduler2 = get_constant_schedule_with_warmup(optimizer2, num_warmup_steps=args.warmup_step_prompt)\n",
    "    \n",
    "elif args.optimizer.lower() == \"adamw\":\n",
    "    # 使用 AdamW 优化器\n",
    "    # 参数:\n",
    "    #   - optimizer_grouped_parameters2: 需要优化的参数组\n",
    "    #   - lr: 学习率,这里设置为 args.prompt_lr,即 Prompt 的学习率\n",
    "    optimizer2 = AdamW(optimizer_grouped_parameters2, lr=args.prompt_lr)\n",
    "    \n",
    "    # 定义学习率调度器,使用线性预热策略\n",
    "    # 参数:\n",
    "    #   - optimizer2: 优化器\n",
    "    #   - num_warmup_steps: 预热步数,这里设置为 args.warmup_step_prompt\n",
    "    #   - num_training_steps: 总的训练步数,即 args.max_steps\n",
    "    scheduler2 = get_linear_schedule_with_warmup(optimizer2, num_warmup_steps=args.warmup_step_prompt, num_training_steps=args.max_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_loss = 0  # 总损失\n",
    "log_loss = 0  # 记录损失\n",
    "best_val_acc = 0  # 最佳验证准确率\n",
    "val_acc_start = 0  # 验证准确率的起始值\n",
    "glb_step = 0  # 全局步数\n",
    "actual_step = 0  # 实际步数\n",
    "leave_training = False  # 是否离开训练\n",
    "tot_train_time = 0  # 总训练时间\n",
    "pbar_update_freq = 10  # 进度条更新频率\n",
    "\n",
    "prompt_model.train()\n",
    "\n",
    "# 在训练前评估模型在验证集上的性能\n",
    "val_res = evaluate(prompt_model, test_dataloader, desc=\"Valid\")\n",
    "print(\"before training, val_res: \", val_res)\n",
    "\n",
    "# 创建进度条,总步数为args.max_steps,描述为\"Train\"\n",
    "pbar = tqdm(total=args.max_steps, desc=\"Train\")\n",
    "\n",
    "# 训练循环,当global_step小于等于args.max_steps时继续训练\n",
    "while glb_step <= args.max_steps:\n",
    "    # 遍历训练数据集\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # 记录训练时间\n",
    "        tot_train_time -= time.time()\n",
    "        \n",
    "        # 前向传播,获取模型输出\n",
    "        logits = prompt_model(inputs)\n",
    "        \n",
    "        # 获取标签\n",
    "        labels = inputs['label']\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = loss_func(logits, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 累加损失\n",
    "        tot_loss += loss.item()\n",
    "        actual_step += 1\n",
    "        \n",
    "        # 如果actual_step能够被gradient_accumulation_steps整除,则执行优化步骤\n",
    "        # 这里是用梯度累积的方式进行训练，即每gradient_accumulation_steps个step更新一次参数\n",
    "        # 这样能够减少显存的使用,但是会增加训练时间\n",
    "        if actual_step % gradient_accumulation_steps == 0:\n",
    "            # 梯度裁剪,最大范数为1.0\n",
    "            torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
    "            \n",
    "            # 更新全局步数\n",
    "            glb_step += 1\n",
    "            \n",
    "            # 如果glb_step能够被pbar_update_freq整除,则更新进度条\n",
    "            if glb_step % pbar_update_freq == 0:\n",
    "                # 计算平均损失\n",
    "                aveloss = (tot_loss - log_loss) / pbar_update_freq\n",
    "                \n",
    "                # 更新进度条\n",
    "                pbar.update(10)\n",
    "                pbar.set_postfix({'loss': aveloss})\n",
    "                \n",
    "                # 重置log_loss\n",
    "                log_loss = tot_loss\n",
    "\n",
    "        # 如果优化器不为None,则执行优化步骤\n",
    "        if optimizer1 is not None:\n",
    "            optimizer1.step()\n",
    "            optimizer1.zero_grad()\n",
    "        if scheduler1 is not None:\n",
    "            scheduler1.step()\n",
    "        if optimizer2 is not None:\n",
    "            optimizer2.step()\n",
    "            optimizer2.zero_grad()\n",
    "        if scheduler2 is not None:\n",
    "            scheduler2.step()\n",
    "\n",
    "        # 累加训练时间\n",
    "        tot_train_time += time.time()\n",
    "\n",
    "        # 如果满足以下条件,则在验证集上评估模型:\n",
    "        # 1. actual_step能够被gradient_accumulation_steps整除\n",
    "        # 2. glb_step大于0\n",
    "        # 3. glb_step能够被args.eval_every_steps整除\n",
    "        if actual_step % gradient_accumulation_steps == 0 and glb_step > 0 and glb_step % args.eval_every_steps == 0:\n",
    "            # 在验证集上评估模型\n",
    "            val_res, labels, preds = evaluate(prompt_model, test_dataloader, desc=\"Valid\", return_data=True)\n",
    "            \n",
    "            # 解包验证结果\n",
    "            acc, p, r, f1 = val_res\n",
    "            \n",
    "            # 保存验证结果\n",
    "            statistics = [args.dataset, args.model_name_or_path, args.seed, glb_step, acc, p, r, f1]\n",
    "            \n",
    "            # 打印训练步数、验证准确率和平均训练时间\n",
    "            print(\"Glb_step {}, val_acc {}, average time {}\".format(glb_step, val_res, tot_train_time / actual_step), flush=True)\n",
    "            \n",
    "            # 将模型设置为训练模式\n",
    "            prompt_model.train()\n",
    "            \n",
    "            # 将验证结果保存到文件\n",
    "            with open(\"sfs_out/task1/%s_%s_%s_%s.pkl\" % (args.dataset, args.model_name_or_path, args.seed, glb_step), \"wb\") as wf:\n",
    "                pkl.dump({\"statistics\": statistics, \"labels\": labels, \"preds\": preds}, wf)\n",
    "            \n",
    "            # 将验证结果写入CSV文件\n",
    "            with open(\"sfs_out/toxic_classification_result.csv\", \"a\") as wf:\n",
    "                wf.write(\"%s,%s,%s,%s,%s,%s,%s,%s\\n\" % (args.dataset, args.model_name_or_path, args.seed, glb_step, acc, p, r, f1))\n",
    "\n",
    "            # 保存模型\n",
    "            save_path = \"saved_models/%s/%s/%s/\" % (args.dataset, args.model, args.model_name_or_path)\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            torch.save(prompt_model.state_dict(), \"%s%s.ckpt\" % (save_path, glb_step))\n",
    "\n",
    "        # 如果glb_step超过了args.max_steps,则设置leave_training为True,表示要离开训练循环\n",
    "        if glb_step > args.max_steps:\n",
    "            leave_training = True\n",
    "            break\n",
    "    \n",
    "    # 如果leave_training为True,则跳出训练循环\n",
    "    if leave_training:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
